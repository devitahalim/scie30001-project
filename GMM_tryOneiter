from cmath import pi
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import style
style.use('fivethirtyeight')
from scipy.stats import norm
from sklearn.mixture import GaussianMixture

np.random.seed(0)

#Generate data cluster
X=np.linspace(-5,5,num=20)
X0=X*np.random.rand(len(X))+10
X1=X*np.random.rand(len(X))-10
#Combine the two clusters
X_total=np.stack((X0,X1)).flatten()



"""
E-step
"""

#Create array with dim nxK
r=np.zeros((len(X_total),2))

gaus_1=norm(loc=-5,scale=5)
gaus_2=norm(loc=8,scale=3)

m = np.array([1/2,1/2]) # We expect to have three clusters 

pi = m/np.sum(m)
#Probability that each data point belongs to each gaussian 
for c,g,p in zip(range(2),[gaus_1,gaus_2],pi):
    r[:,c]=p*g.pdf(X_total) 

#Normalize the probability, each row of r sums to 1 (each row i in r gives probability for x_i)
for i in range(len(r)):
    r[i]=r[i]/(np.sum(pi)*np.sum(r,axis=1)[i])

"""
M-Step
"""
#Calculate total weight m_c
m_c=[]
for c in range (len(r[0])):
    m=np.sum(r[:,c])
    m_c.append(m)

#Calculate pi_c
pi_c=[]
for m in m_c:
    pi_c.append(m/np.sum(m_c))

#Calcuate mu_c
mu_c=np.sum(X_total.reshape(len(X_total),1)*r,axis=0)/m_c

#Calculate var_c
var_c=[]
for c in range(len(r[0])):
    var_c.append((1/m_c[c])*np.dot(((np.array(r[:,c]).reshape(40,1))*(X_total.reshape(len(X_total),1)-mu_c[c])).T,
    (X_total.reshape(len(X_total),1)-mu_c[c])))

#Update gaussians
gaus_1=norm(loc=mu_c[0],scale=var_c[0])
gaus_2=norm(loc=mu_c[1],scale=var_c[1])

#Plotting the data
fig=plt.figure(figsize=(5,5))
ax0=fig.add_subplot(111)

for i in range(len(r)):
    c=np.array([r[i][0],r[i][1]])
    print(c)
    
for i in range(len(r)):
    ax0.scatter(X_total[i],0,c='red',s=100)

#Plotting the Gaussian
for g,c in zip([gaus_1.pdf(np.sort(X_total).reshape(40,1)),gaus_2.pdf(np.sort(X_total).reshape(40,1))],['r','b']):
    ax0.plot(np.sort(X_total),g,c=c)

plt.show()